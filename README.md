# ğŸ“Š Pipeline de qualitÃ© des donnÃ©es â€” Fintech

Un pipeline end-to-end qui ingÃ¨re des transactions financiÃ¨res simulÃ©es,
les transforme avec dbt, valide leur qualitÃ© avec Great Expectations,
orchestre le tout avec Airflow, et expose un dashboard de monitoring.

## ğŸ—ï¸ Architecture

<!-- Colle ici une capture d'Ã©cran de ton schÃ©ma ou du DAG Airflow -->
```text
Faker (Python) â†’ DuckDB â†’ dbt â†’ Great Expectations â†’ Streamlit
                                        â†‘
                                   Airflow (daily)
```

## ğŸ› ï¸ Stack technique

- **Ingestion** : Python + Faker (donnÃ©es synthÃ©tiques)
- **Stockage** : DuckDB
- **Transformation** : dbt Core (staging â†’ intermediate â†’ marts)
- **QualitÃ©** : Great Expectations (7 rÃ¨gles) + dbt tests
- **Orchestration** : Apache Airflow
- **Visualisation** : Streamlit + Plotly

## ğŸš€ Installation et lancement

### PrÃ©-requis
- Python 3.11+
- Docker Desktop (pour Airflow)

### Lancer le pipeline manuellement
```bash
# 1. Cloner le repo
git clone https://github.com/ton-username/fintech-data-quality-pipeline.git
cd fintech-data-quality-pipeline

# 2. CrÃ©er et activer l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# .\venv\Scripts\Activate  # Windows

# 3. Installer les dÃ©pendances
pip install duckdb dbt-duckdb pandas faker streamlit plotly great-expectations

# 4. GÃ©nÃ©rer les donnÃ©es
python scripts/generate_data.py

# 5. ExÃ©cuter dbt
cd dbt_project && dbt run && dbt test && cd ..

# 6. Lancer la validation
python scripts/run_ge_validation.py

# 7. Voir le dashboard
streamlit run app/dashboard.py
```

### Lancer avec Airflow (orchestration automatique)
```bash
docker-compose up airflow-init
docker-compose up airflow-webserver airflow-scheduler
# Ouvrir http://localhost:8080 (admin/admin)
```

## ğŸ“ˆ Dashboard

<!-- INSÃˆRE ICI TES CAPTURES D'Ã‰CRAN -->

Le dashboard affiche :
- Score global de qualitÃ© des donnÃ©es
- RÃ©sultats des validations Great Expectations
- Anomalies par catÃ©gorie de transaction
- Distribution des montants
- RÃ©partition par statut et par pays
- Ã‰volution temporelle du volume

## ğŸ§ª Data Quality : ce qui est vÃ©rifiÃ©

### Tests dbt
- UnicitÃ© des IDs (transactions, clients)
- Pas de NULL sur les champs critiques
- Valeurs acceptÃ©es pour statut et catÃ©gorie

### Great Expectations
- transaction_id : non null + unique
- amount : entre -50k et 50k (tolÃ©rance 5%)
- amount : non null Ã  95%+
- status : valeurs connues uniquement
- category : valeurs connues uniquement
- country : codes pays valides

## ğŸ“ Structure du projet
```
â”œâ”€â”€ dags/                        # DAG Airflow
â”‚   â””â”€â”€ fintech_pipeline_dag.py
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/             # Nettoyage
â”‚   â”‚   â”œâ”€â”€ intermediate/        # Logique mÃ©tier
â”‚   â”‚   â””â”€â”€ marts/               # Tables finales
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_data.py         # GÃ©nÃ©ration de donnÃ©es
â”‚   â””â”€â”€ run_ge_validation.py     # Validation GE
â”œâ”€â”€ app/
â”‚   â””â”€â”€ dashboard.py             # Dashboard Streamlit
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.md
â”œâ”€â”€ docker-compose.yml           # Airflow
â””â”€â”€ README.md
```

## ğŸ’¡ Ce que j'ai appris

- Structurer un projet dbt avec les conventions staging/intermediate/marts
- Ã‰crire des tests de data quality dÃ©claratifs (dbt) et programmatiques (GE)
- Orchestrer un pipeline avec Airflow via Docker
- Concevoir un dashboard de monitoring orientÃ© data quality

## ğŸ‘¤ Auteure

**Ange Francine FORKOU** â€” Data Engineer & BI Developer